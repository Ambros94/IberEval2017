\section{Conclusions} \label{sec:conclusion}

In this paper we have presented our participation in the IberEval2017 Sta (StanceCat) shared task. Five distinct neural models were explored, in combination with different types of preprocessing.
From the fine tuning process we derived that most of well know pre-processing technique are strongly model dependent, meaning that the preprocessing pipeline has to be optimized depending on the classifier.
Our proposal of a dilation technique for NLP task, the Dilated Kim's model, seems to increase performances of CNN base classifiers.
