\section{Evaluation} \label{sec:evaluation}

In this section we are going to illustrate the evaluation of developed systems regarding the modules design reported in \cref{sec:system}.
First we illustrate the metric proposed by organizers for system's evaluation (\Cref{subsec:metric}), then we outline empirical results produced by a 10-fold cross validation over the given data set (\Cref{subsec:tuning}), finally we report our performance at the shared task (\Cref{subsec:results}).

\subsection{Metrics} \label{subsec:metric}

System evaluation metrics were given by the organizers and reported here in the following \cref{eq:gender,eq:stance,eq:f1macro,eq:f1,eq:precision,eq:recall}. Their choice was to use an $F_{1-macro}$ measure for stance detection, due to class unbalance, while a categorical accuracy for the gender detection.

\begin{multicols}{2}
\begin{equation}  \label{eq:gender}
Gender = accuracy = \frac{\sum TP + \sum TN}{\sum sample}
\end{equation}

\begin{equation}  \label{eq:stance}
Stance = \frac{F_{1-macro}(Favor) + F_{1-macro}(Against)}{2}
\end{equation}

\begin{equation}  \label{eq:f1macro}
F_{1-macro}(L) = \frac{1}{|L|} \displaystyle\sum_{l\in L} F_1(y_l, \hat{y}_l)
\end{equation}	
	
\begin{equation} \label{eq:f1}
F_1 = 2 \cdot \frac{precision \cdot recall }{precision + recall}
\end{equation}

\begin{equation} \label{eq:precision}
precision = \frac{1}{|L|} \displaystyle\sum_{l\in L} Pr(y_l, \hat{y}_l)
\end{equation}

\begin{equation} \label{eq:recall}
recall = \frac{1}{|L|} \displaystyle\sum_{l\in L} R(y_l, \hat{y}_l)
\end{equation}
\end{multicols}

\noindent where $L$ is the set of classes, $y_l$ is the set of correct label and $\hat{y}_l$ is the set of predicted labels.


\subsection{Fine tuning process} \label{subsec:tuning}

Following, we describe the fine tuning process of our proposed model over possible combinations of pre-processing (\Cref{tab:preprocessing}), then we compare Kim's model against our extension (\Cref{tab:dilation}) and finally report the improvement over the use of a \emph{data augmentation} technique (\Cref{tab:augmentation}).
For brevity of information only the evaluation of Dilated Kim's model over Spanish stance detection is reported, in details, the results are calculated from averaging three runs of a 10-fold cross validation over the complete data set.
Nevertheless, the results obtained after the fine tuning process for all the models are reported in \cref{subsec:results}, where their development performances are compared against the ones obtained in the \emph{StanceCat} task.

Notations used in \Cref{tab:preprocessing} refer to the one introduced in \Cref{subsec:preprocessing}, where the listing of a notation means its use for the reported result. 
Regarding the tweet specific pre-processing, all the items have been substituted, with the exception for URL and RW that have been removed. We report the contribution of each analysed pre-processing alone.

\begin{table}[t]
\footnotesize
\caption{Pre-processing fine tuning for the Dilated Kim's model from a three run of 10-fold cross validation over the development set. Results are in terms of average $F_{1-macro}$ score. The processing technique that brought a model's improvement has its result in bold.}
\label{tab:preprocessing}
\centering
\begin{tabular}{l|cccccccccc}
\toprule
\hline
\multirow{2}{*}{Models}		& \multicolumn{10}{ c }{Pre-processing}       \\ 
		& Nothing	& ST	& SW	& URL	& RW	& MT	& HT	& NUM	& EM	& SM	\\
\hline
Dilated Kim		& 0.606		& \textbf{0.615}	& 0.590	&  0.585	& 0.578	&  \textbf{0.610}	& 0.543	&  0.570	& 0.564	& 0.585	\\
\hline
\bottomrule
\end{tabular}
\end{table}

From the analysis of \Cref{tab:preprocessing} some relative observation can be made:
\begin{itemize}
\item Most of common used preprocessing decrease model's performance, meaning that their information can be directly exploited by the model
\item Only stemming and mention removals brought small improvements, therefore they will be used as best tuning for our proposed model.
\end{itemize}



\begin{table}[t]
\footnotesize
\caption{Comparison of Kim's and Dilated Kim respect their best pre-processing tuning for stance\&gender detection task. Results are averaged after three run of 10-fold cross validation over the development set in terms of averaged $F_{1-macro}$ score.}
\label{tab:dilation}
\centering
\begin{tabular}{l|cc|cc}
\toprule
\hline
\multirow{2}{*}{Models}		& \multicolumn{2}{c}{Stance}	& \multicolumn{2}{c}{Gender}\\
\cline{2-5}
							& ES		& CA		& ES		& CA		\\
\hline
Kim							& $0.624 (\pm0.017)$ & $0.630 (\pm0.022)$ & $0.634 (\pm0.011)$ & $0.655 (\pm0.017)$	\\
Dilated Kim					& $0.658 (\pm0.039)$ & $0.659 (\pm0.028)$ & $0.652 (\pm0.013)$ & $0.715 (\pm0.015)$	\\
\hline
\bottomrule
\end{tabular}
\end{table}

From the analysis of \Cref{tab:dilation} we can outline a significant performance's improvement of our proposed model respect the original Kim's model.


Due to the fact that our development data set has few samples, to train our models we decided to apply a \emph{data augmentation} technique that didn't rely over external data rather exploit the word embedding text representation. In details, we applied Gaussian noise to word embeddings and after every convolutional layers, and, to further improve performances, we take advantage of batch normalization.
Results of this technique respect the Dilated Kim's model are reported in \cref{tab:augmentation}.

\begin{table}[h]
\footnotesize
\caption{Data augmentation study for Dilated Kim's model over the Spanish stance detection development dataset. Results are averaged after three run of 10-fold cross validation over the development set in terms of averaged $F_{1-macro}$ score. }
\label{tab:augmentation}
\centering
\begin{tabular}{c|ccc}
\toprule
\hline
System		& Nothing	& Gaussian noise	& Batch normalization	\\
\hline
Dilated Kim	& 0.658 ($\pm$ 0.039) & 0.664 ($\pm$ 0.043)	& 0.675 ($\pm$ 0.049)	\\
\hline
\bottomrule
\end{tabular}
\end{table}


\subsection{Competition results} \label{subsec:results}

For the system's submission, participants where allowed to send more than a model till a maximum of 5 possible runs, therefore in \cref{tab:stance,tab:gender} we report our best performing systems (tuned following the process in \cref{subsec:tuning}) for the StanceCat shared task.

Unfortunately, due to a submission error caught only after the official results were published, we didn't manage to be properly evaluated (the minus simbol in \cref{tab:stance,tab:gender}), therefore after the closing we asked organizers to evaluate some of our model to see how they would had performed (the test columns).

\begin{table}[h]
\footnotesize
\caption{Comparison of the best tuning model for the stance detection respect development and test set. The reported ranking refers to the absolute position over all submissions.}
\label{tab:stance}
\centering
\begin{tabular}{c|cc|cc|cc}
\toprule
\hline
\multirow{3}{*}{System}	& \multicolumn{2}{c|}{Development} & \multicolumn{4}{c}{Test}	\\
\cline{2-7}
						& \multirow{2}{*}{ES}	& \multirow{2}{*}{CA}	& \multicolumn{2}{c|}{ES} & \multicolumn{2}{c}{CA}	\\
\cline{4-7}
						&		&		& Score & Ranking & Score & Ranking \\
\hline
LSTM					& $0.443 (\pm0.012)$ & $0.489 (\pm0.012)$ & - & - & - & - \\
Bi-LSTM					& $0.564 (\pm0.035)$ & $0.566 (\pm0.035)$ & 0.410 & 17/31 & 0.386 & 20/31 \\
CNN						& $0.539 (\pm0.030)$ & $0.566 (\pm0.030)$ & - & - & - & - \\
Kim						& $0.625 (\pm0.019)$ & $0.602 (\pm0.019)$ & - & - & - & - \\
Dilated Kim				& \textbf{0.675 ($\pm$0.049)} & \textbf{0.635 ($\pm$0.049)} & - & - & - & - \\
\hline
\bottomrule
\end{tabular}
\end{table}


\begin{table}[h]
	\footnotesize
	\caption{Comparison of the best tuning model for the gender detection respect development and test set. The reported ranking refers to the absolute position over all submissions.}
	\label{tab:gender}
	\centering
	\begin{tabular}{c|cc|cc|cc}
		\toprule
		\hline
		\multirow{3}{*}{System}	& \multicolumn{2}{c|}{Development} & \multicolumn{4}{c}{Test}	\\
		\cline{2-7}
		& \multirow{2}{*}{ES}	& \multirow{2}{*}{CA}	& \multicolumn{2}{c|}{ES} & \multicolumn{2}{c}{CA}	\\
		\cline{4-7}
		&		&		& Score & Ranking & Score & Ranking \\
		\hline
		LSTM					& $0.579 (\pm0.010)$ & $0.648 (\pm0.008)$ & - & - & - & - \\
		Bi-LSTM					& $0.679 (\pm0.025)$ & $0.766 (\pm0.028)$ & - & - & - & - \\
		\textbf{CNN}			& \textbf{0.756 ($\pm$0.027)} & \textbf{0.810 ($\pm$0.022)} & \textbf{0.736} & \textbf{1/21} & \textbf{0.457} & \textbf{4/17} \\
		Kim						& $0.608 (\pm0.017)$ & $0.715 (\pm0.014)$ & - & - & - & - \\
		Dilated Kim				& $0.649 (\pm0.029)$ & $0.745 (\pm0.039)$ & - & - & - & - \\
		\hline
		\bottomrule
	\end{tabular}
\end{table}

In \cref{tab:stance} (stance detection results), we see that our proposed Dilated Kim's model outperformed both recurrent and convolutional models, giving us insight for future developments.

In \cref{tab:gender} (gender detection results), the best performance is achieved by a simple convolutional neural network, that from the test evaluation should had achieved the best result in the Spanish gender detection task.