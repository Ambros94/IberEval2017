\section{Methods} \label{sec:methods}
To address this text classification task we first tried the most widely used text representations and classifiers.
We tried representations based on lexical features as Bag Of Words \cite{harris1954distributional} with TF-IDF normalization and basic semantic features as Bag Of N-Grams.
As classifiers we tried Random Forest, Decision Trees, Support Vector Machines and MultiLayer Perceptron, but since that results obtaines with the combination of those tecniques were outperformed by neural models, we decided not to report those in the paper.
Bag of words

Bag of n-gram


Random forest
Decision tree
Support vecto machines
Multilayer perceptron.


\section{Representation}

vedi citazioni su word embedding di mark e bag of word da maite


\subsection{Word embedding}

ask paolo lexicon/terminology online vs learnign?


\subsection{N-gram embedding}

Representation was learn only online 


\section{Preprocessing}

ref used tool opackage (put on a .bib)

\subsection{Stemming}

\subsection{Stop words}

\subsection{Cleaning}

\subsubsection{Url.}
\subsubsection{R-W.}

\subsection{Tokenizing}

tokenizer options

\subsubsection{Mentions.}
\subsubsection{Smiles.}
\subsubsection{Emoji.}
\subsubsection{Numbers.}


\section{Models}

 	

\subsection{Neural models}

Start whith the meaning reason behind this model (e.g. LSTM because in translation is a state of the art).

Long short term memory.
Bidirectional long short term memory.

Convolutional neural network.
Hybrid convolutional neural network.

\subsubsection{Fast text.}

same and equal of original but changing the number of layer  (worstening in increasing the number of layer) + adding of the gaussian noise and batch normalization

\subsubsection{KIM.}

Pippo \cite{kim2014convolutional}



from kim model we optimize ttill the following net configuration/topology: description of the net