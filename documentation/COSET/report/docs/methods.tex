\section{Methods} \label{sec:methods}
To address this text classification task we first tried the most widely used text representations and classifiers.
We tried representations based on lexical features as Bag Of Words \cite{harris1954distributional},Bag Of N-Grams (bigrams and trigrams), both with and without with TF-IDF normalization.
As classifiers we tried Random Forest, Decision Trees, Support Vector Machines and MultiLayer Perceptron, but since that results obtaines with the combination of those tecniques were outperformed by neural models, we decided not to report those in the paper.
Bag of words
\section{Representation}

vedi citazioni su word embedding di mark e bag of word da maite



si Ã¨ ottimizzato il parametro del numero di parole da usare come input alla rete neurale, arrivando alla misura di 30

numeri superiore risultano in un eccesso di padding e perdita di informazione della rete. Numeri inferiori perdono troppa informazione

\subsection{Word embedding}

Word embedding is a tecnique where elements (in out case words and n-grams) are mapped into a vector of real numbers.
The sentence is mapped into a matrix with dimension sentence lenght x embedding dimension.
We left the sentence length as a parameter and the best results were obtained with length=30, that's reasonable since the average tweet length in words in 24.
We tried static pretrained vectors \cite{bojanowski2016enriching},learning them during training starting from a random matrix or from the pretrained embeddings.
GIANCARLO DEVO DIRE CHE ESPLORIAMO TUTTE LE COMBINAZIONI?

\subsection{N-gram embedding}
We also tried to learn an embedding for n-grams (bigrams and trigrams), but since the corpus is small n-grams frequencies are very low and the algorithm is not able to learn a valid embedding.
Also there are no pre-trained n-grams available.
Bigrams brought a small improvement, bigger n-grams result in performance decrease.



\section{Preprocessing}
We explored different combinations of twitter pre-processing, as converting some elements such mentions, emoji, smiley, hashtags into constant string (i.e. Tokenize @Ambros and \#atoppe :) $\rightarrow $ Tokenize \$MENTION and \$HASHTAG \$SMILEY ); removeing elements as URLs, reserved words and numbers.
We also measured the contribution of stemming, stopwords and punctuation removal.
To address those pre-processing we used the following \cite{nltk} \cite{tweets-preprocessor}.


\section{Models}

 	

\subsection{Neural models}

Start whith the meaning reason behind this model (e.g. LSTM because in translation is a state of the art).

Long short term memory.
Bidirectional long short term memory.

Convolutional neural network.
Hybrid convolutional neural network.

\subsubsection{Fast text.}

same and equal of original but changing the number of layer  (worstening in increasing the number of layer) + adding of the gaussian noise and batch normalization

\subsubsection{KIM.}

Pippo \cite{kim2014convolutional}



from kim model we optimize ttill the following net configuration/topology: description of the net