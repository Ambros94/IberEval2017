\section{Methods} \label{sec:methods}

Bag of words

Bag of n-gram


Random forest
Decision tree
Support vecto machines
Multilayer perceptron.


\section{Representation}

vedi citazioni su word embedding di mark e bag of word da maite


\subsection{Word embedding}

ask paolo lexicon/terminology online vs learnign?


\subsection{N-gram embedding}

Representation was learn only online 


\section{Preprocessing}

ref used tool opackage (put on a .bib)

\subsection{Stemming}

\subsection{Stop words}

\subsection{Cleaning}

\subsubsection{Url.}
\subsubsection{R-W.}

\subsection{Tokenizing}

tokenizer options

\subsubsection{Mentions.}
\subsubsection{Smiles.}
\subsubsection{Emoji.}
\subsubsection{Numbers.}


\section{Models}

 	

\subsection{Neural models}

Start whith the meaning reason behind this model (e.g. LSTM because in translation is a state of the art).

Long short term memory.
Bidirectional long short term memory.

Convolutional neural network.
Hybrid convolutional neural network.

\subsubsection{Fast text.}

same and equal of original but changing the number of layer  (worstening in increasing the number of layer) + adding of the gaussian noise and batch normalization

\subsubsection{KIM.}

Pippo \cite{kim2014convolutional}

pippo style \citep{kim2014convolutional}

from kim model we optimize ttill the following net configuration/topology: description of the net