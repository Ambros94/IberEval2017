\section{Conclusions} \label{sec:conclusion}

In this paper we have presented our participation in the IberEval2017 Classification Of Spanish Election Tweets (COSET) shared task. Five different neural models were explored, in combination with 11 types of preprocessing.


No preprocessing emerged to be the best with every kind of model, indicating that the preprocessing pipeline optimization has a big impact on results. \textbf{NO BRO fermo un secondo, qui c'è qualcosa che non va, non capisco il senso se no prepro è il best, volevi dire il worst?}

preprocessing strongly influence the performance, no general rule (one pre rule them all, to note that some specific pre could have a huge gap between them 

We also explored static vs non-static word embeddings and non-static vectors initialized with pre-trained vectors on a bigger corpus is the best performing combination.


