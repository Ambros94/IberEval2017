\section{Conclusions} \label{sec:conclusion}

In this paper we have presented our participation in the IberEval2017 Classification Of Spanish Election Tweets (COSET) shared task. Five different neural models were explored, in combination with 11 types of preprocessing.


No preprocessing emerged to be the best with every kind of model, indicating that the preprocessing pipeline optimization has a big impact on results. \textbf{NO BRO fermo un secondo, qui c'è qualcosa che non va, non capisco il senso se no prepro è il best, volevi dire il worst?}

preprocessing strongly influence the performance, no general rule (one pre rule them all, to note that some specific pre could have a huge gap between them 

We also explored static vs non-static word embeddings and non-static vectors initialized with pre-trained vectors on a bigger corpus is the best performing combination.

In this task word order is not important  
Per il TOP no one pre-prepr Every model need a different preprocessing (global vs local)
-> ST+SW wors performance
-> CL URL+RW improve
Using pre-trained word vectors is useful  
Learn vectors during training is usefull (non-static)
